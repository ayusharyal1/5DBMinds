{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Converting categorical data into numbers with Pandas and Scikit-learn\n",
    "#feature extraction. \n",
    "#When it involves a lot of manual work, this is often referred to as feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content= [\"hello How ARE YOU there\", \"How is  ARE everything everyting\",\"I am not sure how this is possible\", \"How are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X_train = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'am',\n",
       " u'are',\n",
       " u'everything',\n",
       " u'everyting',\n",
       " u'hello',\n",
       " u'how',\n",
       " u'is',\n",
       " u'not',\n",
       " u'possible',\n",
       " u'sure',\n",
       " u'there',\n",
       " u'this',\n",
       " u'you']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13)\n",
      "[[0 1 0 0 1 1 0 0 0 0 1 0 1]\n",
      " [0 1 1 1 0 1 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 1 1 1 1 0 1 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 5)\t2\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"How ARE how YOU there\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 1 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Similarity Calculations; Calculate Eculidean Distance between the count vectors of the new post and ll the old posts as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1,v2):\n",
    "    delta= v1-v2\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance\n",
    "\n",
    "def dist_norm(v1,v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta= v1_normalized-v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Post 0 with dist = 0.56: hello How ARE YOU there\n",
      "===Post 1 with dist = 0.99: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.20: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.50: How are you\n",
      "Best post is 3 with dist = 0.5042\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_i = None\n",
    "num_samples = len(content)\n",
    "\n",
    "def best_match(X_train,new_post_vec):\n",
    "    best_dist = sys.maxint\n",
    "    for i in range(0, num_samples):\n",
    "        post = content[i]\n",
    "        if post == new_post:\n",
    "            continue\n",
    "        post_vec = X_train.getrow(i)\n",
    "        #d = dist_raw(post_vec, new_post_vec)\n",
    "        d = dist_norm(post_vec, new_post_vec)\n",
    "        print \"===Post %i with dist = %.2f: %s\"%(i,d,post)\n",
    "        if d< best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "    print \"Best post is %i with dist = %.4f\"%(best_i,best_dist)\n",
    "best_match(X_train, new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 1 0 0 0 0 1 0 1]]\n",
      "[[0 1 0 0 0 2 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train.getrow(0).toarray()\n",
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing less important words\n",
    "#Remove more frequent words that do not help to distinguish netween different texts. \n",
    "#MODIFY YOUR Vectorizer\n",
    "vectorizer2 = CountVectorizer(min_df =1, stop_words='english')\n",
    "sorted(vectorizer2.get_stop_words())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Post 0 with dist = 1.00: hello How ARE YOU there\n",
      "===Post 1 with dist = 1.00: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.00: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.00: How are you\n",
      "Best post is 3 with dist = 0.0000\n"
     ]
    }
   ],
   "source": [
    "X_train2 = vectorizer2.fit_transform(content)\n",
    "vectorizer2.get_feature_names()\n",
    "new_post_vec2 = vectorizer2.transform([new_post])\n",
    "best_match(X_train2, new_post_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'graphic'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLTK to reduce words to their stem i.e. origin\n",
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Use StemmedCountVectorizer to do:\n",
    "1. lower casing the raw post in the preprossing step done in parent calss.\n",
    "2. Extracting all individual words in the tokenization step in parent class.\n",
    "3. Converting each word into its stemmed version.'''\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'everyt', u'hello', u'possibl', u'sure']\n",
      "[[0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 1]\n",
      " [0 0 0 0]]\n",
      "===Post 0 with dist = 1.00: hello How ARE YOU there\n",
      "===Post 1 with dist = 1.00: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.00: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.00: How are you\n",
      "Best post is 3 with dist = 0.0000\n",
      "new post: How ARE how YOU there\n"
     ]
    }
   ],
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "X_train3 = stem_vectorizer.fit_transform(content)\n",
    "print stem_vectorizer.get_feature_names()\n",
    "print X_train3.toarray()\n",
    "\n",
    "new_post_vec3 = stem_vectorizer.transform([new_post])\n",
    "best_match(X_train3, new_post_vec3)\n",
    "print(\"new post:\"),new_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FootNotes:\n",
    "    \n",
    "    What does a rater sees when he rates an android app? == Extrinsic Features\n",
    "    What an android app inherits that influences app rating? == Intrinsic Features\n",
    "    \n",
    "    \n",
    "    Vectors to predict: 1. 5-star count, 4-star count, 3-star-count, 2-star count, 1-star count.\n",
    "    Because, average app-rating depends upon the values of these values. Also on current rating of the app.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read CSV\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy as sp\n",
    "# Use NLTK to reduce words to their stem i.e. origin\n",
    "import nltk.stem\n",
    "# Use NLTK to reduce words to their stem i.e. origin\n",
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Score</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>PublicationDate</th>\n",
       "      <th>AppSize</th>\n",
       "      <th>Name</th>\n",
       "      <th>ContentRating</th>\n",
       "      <th>LastUpdateDate</th>\n",
       "      <th>Instalations</th>\n",
       "      <th>IsTopDeveloper</th>\n",
       "      <th>HaveInAppPurchases</th>\n",
       "      <th>IsFree</th>\n",
       "      <th>Developer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NEWS_AND_MAGAZINES</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Read the most popular newspapers from  Sweden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-08T03:00:00.000Z</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Sweden News</td>\n",
       "      <td>Everyone 10+</td>\n",
       "      <td>2015-07-08T03:00:00.000Z</td>\n",
       "      <td>50 - 100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>News Now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MEDIA_AND_VIDEO</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>Sweden Tv channels guide. Tv Sweden include lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-25T03:00:00.000Z</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Tv Sweden</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>2015-07-25T03:00:00.000Z</td>\n",
       "      <td>5,000 - 10,000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>QSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            Category     Score  \\\n",
       "0           0  NEWS_AND_MAGAZINES  5.000000   \n",
       "1           1     MEDIA_AND_VIDEO  2.882353   \n",
       "\n",
       "                                         Description  Price  \\\n",
       "0  Read the most popular newspapers from  Sweden ...      0   \n",
       "1  Sweden Tv channels guide. Tv Sweden include lo...      0   \n",
       "\n",
       "            PublicationDate  AppSize         Name ContentRating  \\\n",
       "0  2015-07-08T03:00:00.000Z      2.9  Sweden News  Everyone 10+   \n",
       "1  2015-07-25T03:00:00.000Z      2.8    Tv Sweden      Everyone   \n",
       "\n",
       "             LastUpdateDate    Instalations IsTopDeveloper HaveInAppPurchases  \\\n",
       "0  2015-07-08T03:00:00.000Z        50 - 100          False              False   \n",
       "1  2015-07-25T03:00:00.000Z  5,000 - 10,000          False              False   \n",
       "\n",
       "  IsFree Developer  \n",
       "0   True  News Now  \n",
       "1   True       QSC  "
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "app_file = '../data/big-data-csv.csv'\n",
    "appdf = pd.read_csv(app_file,sep=',')\n",
    "appdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NEWS_AND_MAGAZINES\n",
       "1       MEDIA_AND_VIDEO\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_cat = appdf.Category\n",
    "col_cat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Category',\n",
       " 'Score',\n",
       " 'Description',\n",
       " 'Price',\n",
       " 'PublicationDate',\n",
       " 'AppSize',\n",
       " 'Name',\n",
       " 'ContentRating',\n",
       " 'LastUpdateDate',\n",
       " 'Instalations',\n",
       " 'IsTopDeveloper',\n",
       " 'HaveInAppPurchases',\n",
       " 'IsFree',\n",
       " 'Developer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appdf.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEWS_AND_MAGAZINES', 'MEDIA_AND_VIDEO', 'ENTERTAINMENT', 'FINANCE',\n",
       "       'MUSIC_AND_AUDIO', 'TRAVEL_AND_LOCAL', 'EDUCATION', 'BUSINESS',\n",
       "       'PERSONALIZATION', 'TRANSPORTATION', 'SPORTS', 'SOCIAL',\n",
       "       'COMMUNICATION', 'PHOTOGRAPHY', 'LIFESTYLE', 'HEALTH_AND_FITNESS',\n",
       "       'TOOLS', 'PRODUCTIVITY', 'WEATHER', 'BOOKS_AND_REFERENCE',\n",
       "       'GAME_TRIVIA', 'MEDICAL', 'GAME_PUZZLE', 'GAME_CASUAL', 'SHOPPING',\n",
       "       'GAME_MUSIC', 'GAME_ACTION', 'GAME_ARCADE', 'GAME_SIMULATION',\n",
       "       'GAME_CARD', 'GAME_CASINO', 'LIBRARIES_AND_DEMO',\n",
       "       'GAME_EDUCATIONAL', 'GAME_SPORTS', 'GAME_WORD', 'GAME_RACING',\n",
       "       'GAME_ROLE_PLAYING', 'GAME_BOARD', 'COMICS', 'GAME_STRATEGY',\n",
       "       'GAME_ADVENTURE'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col_cat.unique())\n",
    "col_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Use StemmedCountVectorizer to do:\n",
    "1. lower casing the raw post in the preprossing step done in parent calss.\n",
    "2. Extracting all individual words in the tokenization step in parent class.\n",
    "3. Converting each word into its stemmed version.'''\n",
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def vectorize_column(dataframe,column_name,vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        print(\"No Vectorizer is explicitly specified. Using CountVectorizer as default one. \")\n",
    "        column_vectorizer = CountVectorizer(min_df=1)\n",
    "    else:\n",
    "        column_vectorizer = vectorizer\n",
    "    if column_name in dataframe.columns.values.tolist():\n",
    "        column_df = dataframe[column_name]\n",
    "        fmatrix = column_vectorizer.fit_transform(column_df)\n",
    "        print column_vectorizer.get_feature_names()\n",
    "        #print(\"vectorized into matrix of shape\"), fmatrix.toarray().shape\n",
    "        dataframe_f = pd.DataFrame(fmatrix.toarray(), columns=column_vectorizer.get_feature_names())\n",
    "        print(\"formed dataframe of size:(\"),dataframe_f.index.max()+1,\",\", dataframe_f.head(1).shape[1],\")\"\n",
    "        \n",
    "        return dataframe_f, fmatrix, column_vectorizer\n",
    "    else:\n",
    "        print(\"No column found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples in column: 100000\n",
      "vectorized into matrix of shape (100000, 41)\n",
      "formed dataframe of size:( 100000 , 41 )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>books_and_refer</th>\n",
       "      <th>busi</th>\n",
       "      <th>comic</th>\n",
       "      <th>communic</th>\n",
       "      <th>educ</th>\n",
       "      <th>entertain</th>\n",
       "      <th>financ</th>\n",
       "      <th>game_act</th>\n",
       "      <th>game_adventur</th>\n",
       "      <th>game_arcad</th>\n",
       "      <th>...</th>\n",
       "      <th>person</th>\n",
       "      <th>photographi</th>\n",
       "      <th>product</th>\n",
       "      <th>shop</th>\n",
       "      <th>social</th>\n",
       "      <th>sport</th>\n",
       "      <th>tool</th>\n",
       "      <th>transport</th>\n",
       "      <th>travel_and_loc</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   books_and_refer  busi  comic  communic  educ  entertain  financ  game_act  \\\n",
       "0                0     0      0         0     0          0       0         0   \n",
       "1                0     0      0         0     0          0       0         0   \n",
       "2                0     0      0         0     0          1       0         0   \n",
       "3                0     0      0         0     0          0       1         0   \n",
       "4                0     0      0         0     0          0       0         0   \n",
       "\n",
       "   game_adventur  game_arcad   ...     person  photographi  product  shop  \\\n",
       "0              0           0   ...          0            0        0     0   \n",
       "1              0           0   ...          0            0        0     0   \n",
       "2              0           0   ...          0            0        0     0   \n",
       "3              0           0   ...          0            0        0     0   \n",
       "4              0           0   ...          0            0        0     0   \n",
       "\n",
       "   social  sport  tool  transport  travel_and_loc  weather  \n",
       "0       0      0     0          0               0        0  \n",
       "1       0      0     0          0               0        0  \n",
       "2       0      0     0          0               0        0  \n",
       "3       0      0     0          0               0        0  \n",
       "4       0      0     0          0               0        0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#column_vectorizer = CountVectorizer(min_df=1)\n",
    "#column_vectorizer = CountVectorizer(min_df =1, stop_words='english')\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "newfeature, fmatrix, column_vectorizer = vectorize_column(appdf, 'Category', stem_vectorizer)\n",
    "#print column_vectorizer.get_feature_names()\n",
    "newfeature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def dist_norm(v1,v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta= v1_normalized-v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '''Analysis of 'Category' data-columns\n",
    "#Each of the application has only one 'category' so the each of the category is equi-distance from all other.\n",
    "Though similarity of each of the values of category is same, the category-name itself might not effect the rating equally.\n",
    "That is why they are inluded as training features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_i = None\n",
    "\n",
    "def best_match(column_vectorizer, fmatrix,text_to_compare):\n",
    "    n_samples = 100 # fmatrix.shape[0]\n",
    "    best_dist = sys.maxint\n",
    "    vect_to_compare = column_vectorizer.transform(text_to_compare)\n",
    "    for i in range(0, n_samples):\n",
    "        text_in_column = col_cat[i]\n",
    "        if text_in_column == text_to_compare[0]:\n",
    "            continue\n",
    "        vector_for_column_text = fmatrix.getrow(i)\n",
    "        #d = dist_raw(post_vec, new_post_vec)\n",
    "        d = dist_norm(vector_for_column_text, vect_to_compare)\n",
    "        print \"===Category of app- %i with dist = %.2f: %s\"%(i,d,text_in_column)\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "    print \"Best text in category is %i with dist = %.4f\"%(best_i,best_dist)\n",
    "print type([col_cat[4]])\n",
    "#best_match(column_vectorizer,fmatrix, [col_cat[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Analysis of Description Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples in column: 100000\n",
      "vectorized into matrix of shape"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-112-42fb8e50ceb9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m#column_vectorizer = CountVectorizer(min_df =1, stop_words='english')\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mstem_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStemmedCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin_df\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstop_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'english'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mnewfeature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmatrix\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorize_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mappdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Description'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstem_vectorizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;31m#print column_vectorizer.get_feature_names()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnewfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-c5a42b1b0500>\u001b[0m in \u001b[0;36mvectorize_column\u001b[1;34m(dataframe, column_name, vectorizer)\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mfmatrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolumn_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumn_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"n_samples in column:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcolumn_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"vectorized into matrix of shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m         \u001b[0mdataframe_f\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfmatrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolumn_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"formed dataframe of size:(\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdataframe_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\")\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/scipy/sparse/compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/scipy/sparse/coo.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/scipy/sparse/base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#column_vectorizer = CountVectorizer(min_df=1)\n",
    "#column_vectorizer = CountVectorizer(min_df =1, stop_words='english')\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "newfeature, fmatrix, column_vectorizer = vectorize_column(appdf, 'Description', stem_vectorizer)\n",
    "#print column_vectorizer.get_feature_names()\n",
    "newfeature.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Name Field\n",
    "    suggest some of the price for higer number of sale\n",
    "    w1: parameterized loudness of words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000     5000+ Cute Love SMS Collection\n",
      "1003      30000+ Funny Jokes Collection\n",
      "1007     90000+ SMS Messages Collection\n",
      "1008     75000+ SMS Messages Collection\n",
      "1578             DuPont™ Tychem® 4000 S\n",
      "2591        Learn Ukrainian 6,000 Words\n",
      "3945                      Sudoku 10'000\n",
      "3948                 Sudoku 10'000 Free\n",
      "3953                 Sudoku 10'000 Plus\n",
      "4350                     1000 Aventuras\n",
      "6369     フルル大辞典 ～即引き！略語・用語・薬辞典 10,000語～\n",
      "6401                 MedCalc 3000 中文精华版\n",
      "7228        プリンス育成☆マジLOVE3000％ for うたプリ\n",
      "7322                20000 Leagues Slots\n",
      "9173             Indian Recipes 10.000+\n",
      "9320      2000 Piadas Engraçadas Brasil\n",
      "10048    50000 Status Quotes Collection\n",
      "10251                      鬼監督の1000本ノック\n",
      "11380             1000 Recipes in Hindi\n",
      "12334           Desert Race Toyota 1000\n",
      "13523                     Taxxi 4209000\n",
      "13630                مسجات +15000 رسالة\n",
      "13702                    Solitaire 1000\n",
      "14624         Learn Chinese 6,000 Words\n",
      "14627      Learn Chinese 10000 Mandarin\n",
      "15473     Ackmi 2000s Music Trivia Quiz\n",
      "16001        1000+ Mignon SMS Française\n",
      "16037         BigOven: 350,000+ Recipes\n",
      "16062      3000 Adbhut Rahasya in HIndi\n",
      "16478           2000+ Tamil Kavithaigal\n",
      "                      ...              \n",
      "78143     13000 videos english learning\n",
      "78338              1000000+ FREE Ebooks\n",
      "78736      Chinese Library - 2000 Books\n",
      "78739    Hemmabiblioteket - 1000 Böcker\n",
      "78984      1000+ Business Idea and Fund\n",
      "80514          Learn German 6,000 Words\n",
      "81018       Best WhatsApp Status 10000+\n",
      "81252          House of 1000 Doors Full\n",
      "82519                 English 6000 Free\n",
      "82521                 ESPANOL 6000 Free\n",
      "85350         꽁돈 - 3분3000원,문상,틴캐시,돈버는어플\n",
      "85526          かわいい！顔文字9000+（無料かおもじアプリ）\n",
      "85879     Бир кеча кундузда 1000 суннат\n",
      "85976    100000 SMS Collection & Status\n",
      "85979        71000+ Hindi Shayari Dukan\n",
      "85994       50000+ Days SMS Collections\n",
      "85997    25000+ SMS Messages Collection\n",
      "87011              100000+ SMS Messages\n",
      "87779            +1000 Hindi SMS Shayri\n",
      "87970    Zinio: 5000+ Digital Magazines\n",
      "90799            Learn Thai 6,000 Words\n",
      "90871           Learn Greek 6,000 Words\n",
      "91182     真人发音 - 英文5分钟(Eng5)-收纳超过5000例句\n",
      "92206        FooBar 2000 Remote Control\n",
      "92742                       iTYDOM 1000\n",
      "93848                 Teleprompter 2000\n",
      "95193    NonogramZ 1000+ online puzzles\n",
      "95900               1000 Cookie Recipes\n",
      "96105        House of 1000 Doors 2 Free\n",
      "98087                   1000 Adventures\n",
      "Name: Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "col_name = appdf.Name\n",
    "print col_name[col_name.str.contains('000')]\n",
    "\n",
    "\n",
    "#stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "#newfeature, fmatrix, column_vectorizer = vectorize_column(appdf, 'Name', stem_vectorizer)\n",
    "#print column_vectorizer.get_feature_names()\n",
    "#newfeature.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of 'Instalations'\n",
    "    It is range values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        150\n",
      "1      15000\n",
      "2       6000\n",
      "3    1500000\n",
      "4       6000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "col_name = appdf.Instalations\n",
    "def separate_instalation_column(dataframe, column_name,return_data_type_as=None):\n",
    "    \n",
    "    col_name = appdf[column_name]\n",
    "    ls = col_name.str.split('-').str.get(0).str.strip(' ').str.replace(',','') #series object\n",
    "    hs = col_name.str.split('-').str.get(1).str.strip(' ').str.replace(',','') #series object\n",
    "    \n",
    "    if return_data_type_as is float64:\n",
    "        ls = ls.astype(float).fillna(0.0)\n",
    "        hs = hs.astype(float).fillna(0.0)\n",
    "        return ls, hs\n",
    "    else:\n",
    "        return ls, hs\n",
    "    \n",
    "ls, hs = separate_instalation_column(appdf,'Instalations', float64)\n",
    "appdf.installs_ls = ls\n",
    "appdf.installs_hs = hs\n",
    "print appdf.installs_ls.head(5) + appdf.installs_hs.head(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
