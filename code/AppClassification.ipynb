{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Converting categorical data into numbers with Pandas and Scikit-learn\n",
    "#feature extraction. \n",
    "#When it involves a lot of manual work, this is often referred to as feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content= [\"hello How ARE YOU there\", \"How is  ARE everything everyting\",\"I am not sure how this is possible\", \"How are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X_train = vectorizer.fit_transform(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'am',\n",
       " u'are',\n",
       " u'everything',\n",
       " u'everyting',\n",
       " u'hello',\n",
       " u'how',\n",
       " u'is',\n",
       " u'not',\n",
       " u'possible',\n",
       " u'sure',\n",
       " u'there',\n",
       " u'this',\n",
       " u'you']"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13)\n",
      "[[0 1 0 0 1 1 0 0 0 0 1 0 1]\n",
      " [0 1 1 1 0 1 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 1 1 1 1 0 1 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"How ARE YOU there\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 1 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Similarity Calculations; Calculate Eculidean Distance between the count vectors of the new post and ll the old posts as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1,v2):\n",
    "    delta= v1-v2\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance\n",
    "\n",
    "def dist_norm(v1,v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta= v1_normalized-v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9223372036854775807\n",
      "===Post 0 with dist = 0.46: hello How ARE YOU there\n",
      "===Post 1 with dist = 1.05: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.27: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.52: How are you\n",
      "Best post is 0 with dist = 0.4595\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "print best_dist\n",
    "best_i = None\n",
    "num_samples = len(content)\n",
    "\n",
    "def best_match(X_train,new_post_vec):\n",
    "    best_dist = sys.maxint\n",
    "    for i in range(0, num_samples):\n",
    "        post = content[i]\n",
    "        if post == new_post:\n",
    "            continue\n",
    "        post_vec = X_train.getrow(i)\n",
    "        #d = dist_raw(post_vec, new_post_vec)\n",
    "        d = dist_norm(post_vec, new_post_vec)\n",
    "        print \"===Post %i with dist = %.2f: %s\"%(i,d,post)\n",
    "        if d< best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "    print \"Best post is %i with dist = %.4f\"%(best_i,best_dist)\n",
    "best_match(X_train, new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 1 0 0 0 0 1 0 1]]\n",
      "[[0 1 0 0 0 1 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train.getrow(0).toarray()\n",
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing less important words\n",
    "#Remove more frequent words that do not help to distinguish netween different texts. \n",
    "#MODIFY YOUR Vectorizer\n",
    "\n",
    "vectorizer2 = CountVectorizer(min_df =1, stop_words='english')\n",
    "sorted(vectorizer2.get_stop_words())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Post 0 with dist = 1.00: hello How ARE YOU there\n",
      "===Post 1 with dist = 1.00: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.00: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.00: How are you\n",
      "Best post is 3 with dist = 0.0000\n"
     ]
    }
   ],
   "source": [
    "X_train2 = vectorizer2.fit_transform(content)\n",
    "vectorizer2.get_feature_names()\n",
    "new_post_vec2 = vectorizer2.transform([new_post])\n",
    "best_match(X_train2, new_post_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'graphic'"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLTK to reduce words to their stem i.e. origin\n",
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Use StemmedCountVectorizer to do:\n",
    "1. lower casing the raw post in the preprossing step done in parent calss.\n",
    "2. Extracting all individual words in the tokenization step in parent class.\n",
    "3. Converting each word into its stemmed version.'''\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "X_train3 = stem_vectorizer.fit_transform(content)\n",
    "print stem_vectorizer.get_feature_names()\n",
    "print X_train3.toarray()\n",
    "\n",
    "new_post_vec3 = stem_vectorizer.transform([new_post])\n",
    "best_match(X_train3, new_post_vec3)\n",
    "print(\"new post:\"),new_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FootNotes:\n",
    "    \n",
    "    What does a rater sees when he rates an android app? == Extrinsic Features\n",
    "    What an android app inherits that influences app rating? == Intrinsic Features\n",
    "    \n",
    "    \n",
    "    Vectors to predict: 1. 5-star count, 4-star count, 3-star-count, 2-star count, 1-star count.\n",
    "    Because, average app-rating depends upon the values of these values. Also on current rating of the app.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
