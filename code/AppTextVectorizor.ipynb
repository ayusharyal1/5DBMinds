{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Converting categorical data into numbers with Pandas and Scikit-learn\n",
    "#feature extraction. \n",
    "#When it involves a lot of manual work, this is often referred to as feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy as sp\n",
    "import nltk.stem\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import string\n",
    "from collections import Counter\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c8223f323857>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mapp_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/work/naresh/data/big-data-csv.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mappdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mapp_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m','\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mappdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "is_production = False\n",
    "if is_production is False:\n",
    "    app_file = '../data/big-data-csv.csv'\n",
    "else:\n",
    "    app_file = '/work/naresh/data/big-data-csv.csv'\n",
    "    \n",
    "appdf = pd.read_csv(app_file,sep=',')\n",
    "appdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NEWS_AND_MAGAZINES\n",
       "1       MEDIA_AND_VIDEO\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_cat = appdf.Category\n",
    "col_cat.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_samples 100000\n"
     ]
    }
   ],
   "source": [
    "#Total Number of Columns:\n",
    "print(\"n_samples\"),max(appdf.index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Category',\n",
       " 'Score',\n",
       " 'Description',\n",
       " 'Price',\n",
       " 'PublicationDate',\n",
       " 'AppSize',\n",
       " 'Name',\n",
       " 'ContentRating',\n",
       " 'LastUpdateDate',\n",
       " 'Instalations',\n",
       " 'IsTopDeveloper',\n",
       " 'HaveInAppPurchases',\n",
       " 'IsFree',\n",
       " 'Developer']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appdf.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEWS_AND_MAGAZINES', 'MEDIA_AND_VIDEO', 'ENTERTAINMENT', 'FINANCE',\n",
       "       'MUSIC_AND_AUDIO', 'TRAVEL_AND_LOCAL', 'EDUCATION', 'BUSINESS',\n",
       "       'PERSONALIZATION', 'TRANSPORTATION', 'SPORTS', 'SOCIAL',\n",
       "       'COMMUNICATION', 'PHOTOGRAPHY', 'LIFESTYLE', 'HEALTH_AND_FITNESS',\n",
       "       'TOOLS', 'PRODUCTIVITY', 'WEATHER', 'BOOKS_AND_REFERENCE',\n",
       "       'GAME_TRIVIA', 'MEDICAL', 'GAME_PUZZLE', 'GAME_CASUAL', 'SHOPPING',\n",
       "       'GAME_MUSIC', 'GAME_ACTION', 'GAME_ARCADE', 'GAME_SIMULATION',\n",
       "       'GAME_CARD', 'GAME_CASINO', 'LIBRARIES_AND_DEMO',\n",
       "       'GAME_EDUCATIONAL', 'GAME_SPORTS', 'GAME_WORD', 'GAME_RACING',\n",
       "       'GAME_ROLE_PLAYING', 'GAME_BOARD', 'COMICS', 'GAME_STRATEGY',\n",
       "       'GAME_ADVENTURE'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col_cat.unique())\n",
    "col_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "class LemmaTokenizer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        self.wnl = WordNetLemmatizer()\n",
    "        \n",
    "    def __call__(self, doc):\n",
    "        \n",
    "        lowers = doc.lower()\n",
    "        doc = lowers.translate(None,string.punctuation) ##remove the punctuation using the character\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]\n",
    "\n",
    "#example, vect = CountVectorizer(tokenizer=LemmaTokenizer()) \n",
    "\n",
    "best_doc = None\n",
    "best_i = None\n",
    "\n",
    "'''Computes eculidean distance between two normalized vectors v1 and v2'''\n",
    "def dist_norm(v1,v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta= v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance\"\n",
    "\n",
    "def best_match(column_vectorizer, fmatrix,text_to_compare):\n",
    "    n_samples = 100 # fmatrix.shape[0]\n",
    "    best_dist = sys.maxint\n",
    "    vect_to_compare = column_vectorizer.transform(text_to_compare)\n",
    "    for i in range(0, n_samples):\n",
    "        text_in_column = col_cat[i]\n",
    "        if text_in_column == text_to_compare[0]:\n",
    "            continue\n",
    "        vector_for_column_text = fmatrix.getrow(i)\n",
    "        #d = dist_raw(post_vec, new_post_vec)\n",
    "        d = dist_norm(vector_for_column_text, vect_to_compare)\n",
    "        print \"===Category of app- %i with dist = %.2f: %s\"%(i,d,text_in_column)\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "    print \"Best text in category is %i with dist = %.4f\"%(best_i,best_dist)\n",
    "print type([col_cat[4]])\n",
    "#best_match(column_vectorizer,fmatrix, [col_cat[4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Use StemmedCountVectorizer to do:\n",
    "1. lower casing the raw post in the preprossing step done in parent calss.\n",
    "2. Extracting all individual words in the tokenization step in parent class.\n",
    "3. Converting each word into its stemmed version.'''\n",
    "\n",
    "import enchant\n",
    "import re\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    \n",
    "    eng_dict = enchant.Dict(\"en_US\")\n",
    "    filter_option=['OnlyEng','AllLang'] \n",
    "    filter_by =\"\"\n",
    "    no_of_dialect = 0\n",
    "    regex = r'[0-9_=*.-+]+' #Unicode strings are immune to regular expression.\n",
    "    \n",
    "    def setfilter_option(self, filter_by, count_dialect):\n",
    "        self.filter_by = filter_by\n",
    "        self.count_dialect = count_dialect\n",
    "        self.eng_dict = enchant.Dict(\"en_US\")\n",
    "    \n",
    "    ##overiding the analyzer of CountVectorizer\n",
    "    def build_analyzer(self):\n",
    "        #english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: self.analyzeddf(analyzer,doc)#(english_stemmer.stem(w.strip(regex)) for w in analyzer(doc) if w.isdigit() is False)               \n",
    "\n",
    "    #For each document i.e. sentence/row this function is called.\n",
    "    def analyzeddf(self,analyzer,doc):\n",
    "        \n",
    "        english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        tokens= analyzer(doc)\n",
    "        \n",
    "        if self.filter_by=='OnlyEng':\n",
    "            tokens=[token for token in  analyzer(doc) if self.eng_dict.check(token) is True]\n",
    "            \n",
    "        if self.filter_by =='AllLang':\n",
    "            tokens = [re.sub(regex,'',token) for token in tokens]\n",
    "            tokens = [token.strip(\"_\") for token in tokens if token !='']\n",
    "            return (english_stemmer.stem(w) for w in tokens if w.isdigit() is False)\n",
    "        \n",
    "        return (english_stemmer.stem(w) for w in tokens if w.isdigit() is False)\n",
    "        \n",
    "def stat_vectorized_matrix(vectorized_array,vectorizer_type=None):\n",
    "   \n",
    "    #count the number of features generated,\n",
    "    m, n = vectorized_array.shape\n",
    "    count_non_zero_cells = np.count_nonzero(vectorized_array) #vectorized_array.nnz\n",
    "    #print(\"vectorizer_type:\"),type(vectorizer_type)\n",
    "    #print(\"Sparse matrix shape: \"), vectorized_array.shape\n",
    "    #count the number of non-zero entries,\n",
    "    #print(\"Sparsity(%%of non-zero values): %.6f %%\" %(count_non_zero_cells/float(m*n) * 100))\n",
    "    sparsity = (count_non_zero_cells/float(m*n) * 100)\n",
    "    vectorizer_type = type(vectorizer_type)\n",
    "    vector_shape = vectorized_array.shape\n",
    "    record = [vectorizer_type,vector_shape, sparsity]\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to Install Stop Words ?\n",
    "\n",
    "    In your terminal:\n",
    "                    $python\n",
    "                    >>import nltk\n",
    "                    >>nltk.download()\n",
    "                    >>d ##hit 'd'\n",
    "                    >>stopwords ##type stopwords\n",
    "    Suppose you downloaded the stopwords in your '~/nltk_data/corpora/stopwords' folder\n",
    "            1. Perhaps the folder is downloaded in your current directory ~/nltk_data/corpora/stopwords\n",
    "            2. Extract 5DBMinds/data/stopwords-extended.zip of our project repository in github.\n",
    "\n",
    "3. Copy all files to your ~/nltk_data/corpora/stopwords folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "'''Returns a vectorized ND dataframe, vectorized ndarray, \n",
    "and an instance of the vectorizer Class used to transform.'''\n",
    "\n",
    "def vectorize_column(dataframe,column_name,vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        print(\"No Vectorizer is explicitly specified. Using CountVectorizer as default one. \")\n",
    "        column_vectorizer = CountVectorizer(min_df=1)\n",
    "    else:\n",
    "        column_vectorizer = vectorizer\n",
    "    \n",
    "    if column_name in dataframe.columns.values.tolist():\n",
    "        column_df = dataframe[column_name] #select all the samples from the column passed as param.\n",
    "        fmatrix = column_vectorizer.fit_transform(column_df) #convert text features to numerical vectors\n",
    "        dataframe_f = pd.DataFrame(fmatrix.toarray(), columns=column_vectorizer.get_feature_names())\n",
    "        print(\"Dataframe shape :(\"),dataframe_f.index.max()+1,\",\", dataframe_f.head(1).shape[1],\")\"\n",
    "        \n",
    "        return dataframe_f, fmatrix, column_vectorizer\n",
    "    else:\n",
    "        print(\"No column found\")\n",
    "\n",
    "#Custom tokenizer used by CountVectorizer.\n",
    "def custom_tokenizer(doc):\n",
    "    \n",
    "    lowers = doc.lower() #lower-casing\n",
    "    lowers = lowers.translate(string.punctuation) #remove punctuation\n",
    "    tokens = nltk.word_tokenize(lowers)\n",
    "    return tokens\n",
    "       \n",
    "'''Returns a vectorized (n_samples,n_features) dataframe, matrix and vectorizing object.\n",
    "Parameters:\n",
    "dataframe: pandas dataframe object\n",
    "column_name: name of the column you want to vectorize (a column in above dataframe object)\n",
    "vectorizer= Vectorizer Object, if none then CountVectorizer is used as default. \n",
    "n_samples: number of rows you want to vectorize\n",
    "tf_idf: if True then TF-IDF matrix is returned, else only matrix of term frequency is return.\n",
    "\n",
    "USAGE:\n",
    "stem_vectorizer = StemmedCountVectorizer(encoding='utf-8',\n",
    "                                         min_df =min_df,\n",
    "                                         max_df =max_df,\n",
    "                                         stop_words='english',\n",
    "                                         analyzer='word',\n",
    "                                         lowercase = lowercase)\n",
    "                                         filter_by = ['OnlyEng','AllLang']\n",
    "##set filterparameter to your vectorizer\n",
    "filter_by=[\"OnlyEng\", \"AllLang\"] #two options are available\n",
    "count_dialect = True \n",
    "n_samples = n_samples #as u choose it.\n",
    "stem_vectorizer.setfilter_option(filter_by[0],count_dialect)\n",
    "\n",
    "dfx, matrixX, sv = vectorize_columnTfIdf(df, 'my_column',vectorizer=stem_vectorizer, n_samples=100, tf_idf=True)\n",
    "'''\n",
    "\n",
    "def vectorize_columnTfIdf(dataframe,column_name,vectorizer=None, n_samples=None, tf_idf=False):\n",
    "    \n",
    "    more_stopwords = []\n",
    "    more_stopwords  = ['00','000','0000','0003','0004','0004','0005'] \n",
    "    more_stopwords += stopwords.words('english')\n",
    "    more_stopwords += stopwords.words('japanese') \n",
    "    more_stopwords += stopwords.words('chinese')\n",
    "    more_stopwords += stopwords.words('arabic')\n",
    "    more_stopwords += stopwords.words('korean')\n",
    "    more_stopwords += stopwords.words('russian')    \n",
    "    \n",
    "    if vectorizer is None:\n",
    "        print(\"No Vectorizer is explicitly specified. Using CountVectorizer as default one. \")\n",
    "        column_vectorizer = CountVectorizer(min_df=1, \n",
    "                                            max_df= 0.99, \n",
    "                                            stop_words=more_stopwords) #default vectorizer\n",
    "    else:\n",
    "        column_vectorizer = vectorizer\n",
    "        column_vectorizer.stop_words = more_stopwords\n",
    "    \n",
    "    if column_name in dataframe.columns.values.tolist():\n",
    "        \n",
    "        if n_samples is None:\n",
    "            column_df = dataframe[column_name] #select all the samples from the column passed as param. \n",
    "            print len(column_df)\n",
    "        else:\n",
    "            #column_df = dataframe[column_name].iloc[:n_samples] #select all the samples from the column passed as param.\n",
    "            column_df = dataframe[column_name].iloc[:n_samples] \n",
    "        fmatrix = column_vectorizer.fit_transform(column_df)   \n",
    "        \n",
    "        if(tf_idf is True):\n",
    "            \n",
    "            tfidf_transformer  = TfidfTransformer(norm='l2').fit(fmatrix)\n",
    "            tfidfNormalzedmatrix = tfidf_transformer.transform(fmatrix)\n",
    "            fmatrix = tfidfNormalzedmatrix\n",
    "            \n",
    "        dataframe_f = pd.DataFrame(fmatrix.todense(), columns=column_vectorizer.get_feature_names())\n",
    "        print(\"formed dataframe of size:(\"),dataframe_f.index.max()+1,\",\", dataframe_f.head(1).shape[1],\")\"\n",
    "        \n",
    "        return dataframe_f, fmatrix, column_vectorizer\n",
    "    else:\n",
    "        print(\"No column found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove following words:\n",
    "\n",
    "    5. Do capital letters carry information? [Lowercasing]\n",
    "    4. Does distinguishing inflected form (\"goes\" vs. \"go\") carry information?[Stemming/Lemmantizing]\n",
    "    3. Do interjections, determiners carry information (Stop Words)?\n",
    "    2. Does numerical strings carries information? 000, 000, 100 \t000, 0000,000031,0002, 03 ,004,\t0005 \t0006 \t0007\n",
    "    1. \n",
    "####  Term Frequency: \n",
    "    Counting how many times does a word occur in each message (Term Freq.)\n",
    "#### Inverse  Document Frequency:\n",
    "    weighting the counts, so that frequent tokens get lower weight \n",
    "#### Normalization\n",
    "    normalizing the vectors to unit length, to abstract from the original text length (L2 Norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CATEGORY\n",
    "#### '''Analysis of 'Category' data-columns\n",
    "##### Each of the application has only one 'category' so the each of the category is equi-distance from all other.\n",
    "Though similarity of each of the values of category is same, the category-name itself might not effect the rating equally.\n",
    "That is why they are inluded as training features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python2.7/site-packages/ipykernel/ipkernel.py\", line 175, in do_execute\n    shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/usr/lib/python2.7/site-packages/IPython/core/interactiveshell.py\", line 2832, in run_cell\n    cell = self.input_transformer_manager.transform_cell(raw_cell)\n",
      "  File \"/usr/lib/python2.7/site-packages/IPython/core/inputsplitter.py\", line 597, in transform_cell\n    self.push(cell)\n",
      "  File \"/usr/lib/python2.7/site-packages/IPython/core/inputsplitter.py\", line 641, in push\n    out = self.push_line(line)\n",
      "  File \"/usr/lib/python2.7/site-packages/IPython/core/inputsplitter.py\", line 668, in push_line\n    line = self.assemble_python_lines.push(line)\n",
      "  File \"/usr/lib/python2.7/site-packages/IPython/core/inputtransformer.py\", line 151, in push\n    for intok in self.tokenizer:\n",
      "  File \"/usr/lib/python2.7/site-packages/IPython/utils/_tokenize_py2.py\", line 303, in generate_tokens\n    line = readline()\n",
      "  File \"/usr/lib/python2.7/site-packages/IPython/core/inputtransformer.py\", line 136, in get_line\n    def get_line(self):\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "'''Possible type of count vectorizer that could be used.\n",
    "    Examples: \n",
    "    column_vectorizer = CountVectorizer(min_df=1)\n",
    "    column_vectorizer = CountVectorizer(min_df =1, stop_words='english') \n",
    "    print column_vectorizer.get_feature_names()\n",
    "    Do not ASSIGN max_df and min_df if you are using TF-IDF. Because tf-idf considers the case.\n",
    "'''\n",
    "print(\"CATEGORY\")\n",
    "min_df = 1\n",
    "max_df = 0.99 #it's value lies in: [0.7, 1.0), remove the word that occur in more than 90% of all the posts.\n",
    "token_pattern = r\"\\b[a-z]\\b\"\n",
    "lowercase = True\n",
    "stem_vectorizer = StemmedCountVectorizer(encoding='utf-8',\n",
    "                                         min_df =min_df,\n",
    "                                         max_df =max_df,\n",
    "                                         stop_words='english',\n",
    "                                         analyzer='word',\n",
    "                                         lowercase = lowercase)\n",
    "\n",
    "\n",
    "column_name = 'Category'\n",
    "filter_by = ['OnlyEng','AllLang']\n",
    "count_dialect = True\n",
    "n_samples = 70\n",
    "record = []\n",
    "vectorizers_list = [None,stem_vectorizer]\n",
    "for vectorizer in vectorizers_list:\n",
    "    \n",
    "    for filter_by_opt in filter_by:\n",
    "\n",
    "        stem_vectorizer.setfilter_option(filter_by_opt,count_dialect)\n",
    "        cat_newfeature, cat_fmatrix, cat_column_vectorizer = vectorize_columnTfIdf(appdf, column_name,vectorizer=vectorizer)\n",
    "        row = stat_vectorized_matrix(cat_fmatrix.toarray(), cat_column_vectorizer)\n",
    "        row.insert(0,column_name)\n",
    "        row.insert(1,filter_by_opt)\n",
    "        record.append(row)\n",
    "    \n",
    "print record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings:\n",
    "\n",
    "    Only English words exists. The words may be separated by a '_'.\n",
    "    Since each of the app corresponds to a category, use Use 'AllLang' options when vectorizing \n",
    "    'Category' Field.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Description Field\n",
    "#### Analysis of Description Fields\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Vectorizer is explicitly specified. Using CountVectorizer as default one. \n",
      "formed dataframe of size:( 70 , 2900 )\n",
      "No Vectorizer is explicitly specified. Using CountVectorizer as default one. \n",
      "formed dataframe of size:( 70 , 2900 )\n",
      "formed dataframe of size:( 70 , 1232 )\n",
      "formed dataframe of size:( 70 , 2417 )\n",
      "[['Description', 'OnlyEng', <class 'sklearn.feature_extraction.text.CountVectorizer'>, (70, 2900), 2.5600985221674875], ['Description', 'AllLang', <class 'sklearn.feature_extraction.text.CountVectorizer'>, (70, 2900), 2.5600985221674875], ['Description', 'OnlyEng', <class '__main__.StemmedCountVectorizer'>, (70, 1232), 3.5760667903525047], ['Description', 'AllLang', <class '__main__.StemmedCountVectorizer'>, (70, 2417), 2.843548673089426]]\n"
     ]
    }
   ],
   "source": [
    "#token_pattern = r\"\\b[a-z]*\\b\"\n",
    "\n",
    "token_pattern = r\"*\"\n",
    "\n",
    "col_desc = appdf.Description\n",
    "df_desc = pd.DataFrame(col_desc, columns=['Description']).iloc[:]\n",
    "\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df =min_df,\n",
    "                                         max_df= max_df,\n",
    "                                         analyzer='word'\n",
    "                                         )\n",
    "\n",
    "\n",
    "column_name = 'Description'\n",
    "\n",
    "filter_by = ['OnlyEng','AllLang']\n",
    "count_dialect = True\n",
    "n_samples = 100000\n",
    "\n",
    "vectorizers_list = [None,stem_vectorizer]\n",
    "\n",
    "for vectorizer in vectorizers_list:\n",
    "    \n",
    "    for filter_by_opt in filter_by:\n",
    "        stem_vectorizer.setfilter_option(filter_by_opt,count_dialect)\n",
    "        desc_newfeature, desc_fmatrix, desc_column_vectorizer = vectorize_columnTfIdf(appdf, column_name,vectorizer=vectorizer, n_samples=n_samples)\n",
    "        row = stat_vectorized_matrix(desc_fmatrix.toarray(), desc_column_vectorizer)\n",
    "        row.insert(0,column_name)\n",
    "        row.insert(1,filter_by_opt)\n",
    "        record.append(row)\n",
    "        \n",
    "print record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print desc_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find out which Languages are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import pycountry\n",
    "\n",
    "lang_code_list = []\n",
    "\n",
    "feature_names = desc_column_vectorizer.get_feature_names()\n",
    "\n",
    "for word in feature_names:\n",
    "    \n",
    "    lang_found = detect(word)\n",
    "    if lang_found not in lang_list:\n",
    "        lang_code_list += [lang_found]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pycountry\n",
    "\n",
    "lang_name_list = []\n",
    "for lang in lang_code_list:\n",
    "    try:\n",
    "        print lang.upper()\n",
    "        namec= pycountry.countries.get(alpha2=lang.upper()).name\n",
    "        lang_name_list +=[namec]\n",
    "    except:\n",
    "        print(\"Error on:\"), lang    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In an average, one cell of a description column generates 57 features. In this way, there are 57*n_samples features geneated after vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of Name Field\n",
    "    suggest some of the price for higer number of sale\n",
    "    w1: parameterized loudness of words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Vectorizer is explicitly specified. Using CountVectorizer as default one. \n",
      "formed dataframe of size:( 5000 , 5277 )\n",
      "No Vectorizer is explicitly specified. Using CountVectorizer as default one. \n",
      "formed dataframe of size:( 5000 , 5277 )\n",
      "formed dataframe of size:( 5000 , 1955 )\n",
      "formed dataframe of size:( 5000 , 4849 )\n",
      "[['Name', 'OnlyEng', <class 'sklearn.feature_extraction.text.CountVectorizer'>, (5000, 5277), 0.05299223043395869], ['Name', 'AllLang', <class 'sklearn.feature_extraction.text.CountVectorizer'>, (5000, 5277), 0.05299223043395869], ['Name', 'OnlyEng', <class '__main__.StemmedCountVectorizer'>, (5000, 1955), 0.08113554987212276], ['Name', 'AllLang', <class '__main__.StemmedCountVectorizer'>, (5000, 4849), 0.05679521550835223]]\n"
     ]
    }
   ],
   "source": [
    "col_name = appdf.Name\n",
    "df_desc = pd.DataFrame(col_desc, columns=['Name']).iloc[:]\n",
    "\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df =min_df,\n",
    "                                         max_df= max_df,\n",
    "                                         analyzer='word'\n",
    "                                         )\n",
    "column_name = 'Name'\n",
    "filter_by = ['OnlyEng','AllLang']\n",
    "count_dialect = True\n",
    "n_samples = 5000\n",
    "\n",
    "vectorizers_list = [None,stem_vectorizer]\n",
    "\n",
    "for vectorizer in vectorizers_list:\n",
    "    \n",
    "    for filter_by_opt in filter_by:\n",
    "        stem_vectorizer.setfilter_option(filter_by_opt,count_dialect)\n",
    "        name_newfeature, name_fmatrix, name_column_vectorizer = vectorize_columnTfIdf(appdf, column_name,vectorizer=vectorizer, n_samples=n_samples)\n",
    "        row = stat_vectorized_matrix(name_fmatrix.toarray(), name_column_vectorizer)\n",
    "        row.insert(0,column_name)\n",
    "        row.insert(1,filter_by_opt)\n",
    "        record.append(row)\n",
    "print record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of 'Instalations'\n",
    "    It is range values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "col_name = appdf.Instalations\n",
    "def separate_instalation_column(dataframe, column_name,return_data_type_as=None):\n",
    "    \n",
    "    col_name = appdf[column_name]\n",
    "    ls = col_name.str.split('-').str.get(0).str.strip(' ').str.replace(',','') #series object\n",
    "    hs = col_name.str.split('-').str.get(1).str.strip(' ').str.replace(',','') #series object\n",
    "    \n",
    "    if return_data_type_as is float64:\n",
    "        ls = ls.astype(float).fillna(0.0)\n",
    "        hs = hs.astype(float).fillna(0.0)\n",
    "        return ls, hs\n",
    "    else:\n",
    "        return ls, hs\n",
    "    \n",
    "ls, hs = separate_instalation_column(appdf,'Instalations', float64)\n",
    "appdf.installs_ls = ls\n",
    "appdf.installs_hs = hs\n",
    "print appdf.installs_ls.head(5) + appdf.installs_hs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues:\n",
    "            Tokenization problem\n",
    "            Vectorization problem memory error\n",
    "            Plot the frequency distribution of price \n",
    "            Do linear regression on price and plot Predicted_price-Desired_price Vs Predicted_price\n",
    "            \n",
    "            \n",
    "            http://www.cs.toronto.edu/~marlin/research/thesis/cfmlp.pdf\n",
    "            \n",
    "### FootNotes:\n",
    "    \n",
    "        What does a rater sees when he rates an android app? == Extrinsic Features\n",
    "        What an android app inherits that influences app rating? == Intrinsic Features\n",
    "\n",
    "\n",
    "        Vectors to predict: 1. 5-star count, 4-star count, 3-star-count, 2-star count, 1-star count.\n",
    "        Because, average app-rating depends upon the values of these values. Also on current rating of the app.\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
