{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#Converting categorical data into numbers with Pandas and Scikit-learn\n",
    "#feature extraction. \n",
    "#When it involves a lot of manual work, this is often referred to as feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "content= [\"hello How ARE YOU there\", \"How is  ARE everything everyting\",\"I am not sure how this is possible\", \"How are you\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(min_df=1)\n",
    "X_train = vectorizer.fit_transform(content)s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'am',\n",
       " u'are',\n",
       " u'everything',\n",
       " u'everyting',\n",
       " u'hello',\n",
       " u'how',\n",
       " u'is',\n",
       " u'not',\n",
       " u'possible',\n",
       " u'sure',\n",
       " u'there',\n",
       " u'this',\n",
       " u'you']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 13)\n",
      "[[0 1 0 0 1 1 0 0 0 0 1 0 1]\n",
      " [0 1 1 1 0 1 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 1 1 1 1 0 1 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train.shape\n",
    "print X_train.toarray()ssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 1)\t1\n",
      "  (0, 5)\t2\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "new_post = \"How ARE how YOU there\"\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 0 1 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Similarity Calculations; Calculate Eculidean Distance between the count vectors of the new post and ll the old posts as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy as sp\n",
    "def dist_raw(v1,v2):\n",
    "    delta= v1-v2\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance\n",
    "\n",
    "def dist_norm(v1,v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta= v1_normalized-v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Post 0 with dist = 0.56: hello How ARE YOU there\n",
      "===Post 1 with dist = 0.99: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.20: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.50: How are you\n",
      "Best post is 3 with dist = 0.5042\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_i = None\n",
    "num_samples = len(content)\n",
    "\n",
    "def best_match(X_train,new_post_vec):\n",
    "    best_dist = sys.maxint\n",
    "    for i in range(0, num_samples):\n",
    "        post = content[i]\n",
    "        if post == new_post:\n",
    "            continue\n",
    "        post_vec = X_train.getrow(i)\n",
    "        #d = dist_raw(post_vec, new_post_vec)\n",
    "        d = dist_norm(post_vec, new_post_vec)\n",
    "        print \"===Post %i with dist = %.2f: %s\"%(i,d,post)\n",
    "        if d< best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "    print \"Best post is %i with dist = %.4f\"%(best_i,best_dist)\n",
    "best_match(X_train, new_post_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 0 0 1 1 0 0 0 0 1 0 1]]\n",
      "[[0 1 0 0 0 2 0 0 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "print X_train.getrow(0).toarray()\n",
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'across',\n",
       " 'after',\n",
       " 'afterwards',\n",
       " 'again',\n",
       " 'against',\n",
       " 'all',\n",
       " 'almost']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Removing less important words\n",
    "#Remove more frequent words that do not help to distinguish netween different texts. \n",
    "#MODIFY YOUR Vectorizer\n",
    "vectorizer2 = CountVectorizer(min_df =1, stop_words='english')\n",
    "sorted(vectorizer2.get_stop_words())[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Post 0 with dist = 1.00: hello How ARE YOU there\n",
      "===Post 1 with dist = 1.00: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.00: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.00: How are you\n",
      "Best post is 3 with dist = 0.0000\n"
     ]
    }
   ],
   "source": [
    "X_train2 = vectorizer2.fit_transform(content)\n",
    "vectorizer2.get_feature_names()\n",
    "new_post_vec2 = vectorizer2.transform([new_post])\n",
    "best_match(X_train2, new_post_vec2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'graphic'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use NLTK to reduce words to their stem i.e. origin\n",
    "import nltk.stem\n",
    "s= nltk.stem.SnowballStemmer('english')\n",
    "s.stem(\"graphics\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Use StemmedCountVectorizer to do:\n",
    "1. lower casing the raw post in the preprossing step done in parent calss.\n",
    "2. Extracting all individual words in the tokenization step in parent class.\n",
    "3. Converting each word into its stemmed version.'''\n",
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'everyt', u'hello', u'possibl', u'sure']\n",
      "[[0 1 0 0]\n",
      " [1 0 0 0]\n",
      " [0 0 1 1]\n",
      " [0 0 0 0]]\n",
      "===Post 0 with dist = 1.00: hello How ARE YOU there\n",
      "===Post 1 with dist = 1.00: How is  ARE everything everyting\n",
      "===Post 2 with dist = 1.00: I am not sure how this is possible\n",
      "===Post 3 with dist = 0.00: How are you\n",
      "Best post is 3 with dist = 0.0000\n",
      "new post: How ARE how YOU there\n"
     ]
    }
   ],
   "source": [
    "stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "X_train3 = stem_vectorizer.fit_transform(content)\n",
    "print stem_vectorizer.get_feature_names()\n",
    "print X_train3.toarray()\n",
    "\n",
    "new_post_vec3 = stem_vectorizer.transform([new_post])\n",
    "best_match(X_train3, new_post_vec3)\n",
    "print(\"new post:\"),new_post"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FootNotes:\n",
    "    \n",
    "    What does a rater sees when he rates an android app? == Extrinsic Features\n",
    "    What an android app inherits that influences app rating? == Intrinsic Features\n",
    "    \n",
    "    \n",
    "    Vectors to predict: 1. 5-star count, 4-star count, 3-star-count, 2-star count, 1-star count.\n",
    "    Because, average app-rating depends upon the values of these values. Also on current rating of the app.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Read CSV\n",
    "import pandas as pd\n",
    "from pandas import *\n",
    "from numpy import *\n",
    "import numpy as np\n",
    "import os\n",
    "from pandas import DataFrame\n",
    "import numpy\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import scipy as sp\n",
    "# Use NLTK to reduce words to their stem i.e. origin\n",
    "import nltk.stem\n",
    "# Use NLTK to reduce words to their stem i.e. origin\n",
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Category</th>\n",
       "      <th>Score</th>\n",
       "      <th>Description</th>\n",
       "      <th>Price</th>\n",
       "      <th>PublicationDate</th>\n",
       "      <th>AppSize</th>\n",
       "      <th>Name</th>\n",
       "      <th>ContentRating</th>\n",
       "      <th>LastUpdateDate</th>\n",
       "      <th>Instalations</th>\n",
       "      <th>IsTopDeveloper</th>\n",
       "      <th>HaveInAppPurchases</th>\n",
       "      <th>IsFree</th>\n",
       "      <th>Developer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NEWS_AND_MAGAZINES</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>Read the most popular newspapers from  Sweden ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-08T03:00:00.000Z</td>\n",
       "      <td>2.9</td>\n",
       "      <td>Sweden News</td>\n",
       "      <td>Everyone 10+</td>\n",
       "      <td>2015-07-08T03:00:00.000Z</td>\n",
       "      <td>50 - 100</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>News Now</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>MEDIA_AND_VIDEO</td>\n",
       "      <td>2.882353</td>\n",
       "      <td>Sweden Tv channels guide. Tv Sweden include lo...</td>\n",
       "      <td>0</td>\n",
       "      <td>2015-07-25T03:00:00.000Z</td>\n",
       "      <td>2.8</td>\n",
       "      <td>Tv Sweden</td>\n",
       "      <td>Everyone</td>\n",
       "      <td>2015-07-25T03:00:00.000Z</td>\n",
       "      <td>5,000 - 10,000</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>QSC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            Category     Score  \\\n",
       "0           0  NEWS_AND_MAGAZINES  5.000000   \n",
       "1           1     MEDIA_AND_VIDEO  2.882353   \n",
       "\n",
       "                                         Description  Price  \\\n",
       "0  Read the most popular newspapers from  Sweden ...      0   \n",
       "1  Sweden Tv channels guide. Tv Sweden include lo...      0   \n",
       "\n",
       "            PublicationDate  AppSize         Name ContentRating  \\\n",
       "0  2015-07-08T03:00:00.000Z      2.9  Sweden News  Everyone 10+   \n",
       "1  2015-07-25T03:00:00.000Z      2.8    Tv Sweden      Everyone   \n",
       "\n",
       "             LastUpdateDate    Instalations IsTopDeveloper HaveInAppPurchases  \\\n",
       "0  2015-07-08T03:00:00.000Z        50 - 100          False              False   \n",
       "1  2015-07-25T03:00:00.000Z  5,000 - 10,000          False              False   \n",
       "\n",
       "  IsFree Developer  \n",
       "0   True  News Now  \n",
       "1   True       QSC  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "app_file = '../data/big-data-csv.csv'\n",
    "appdf = pd.read_csv(app_file,sep=',')\n",
    "appdf.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    NEWS_AND_MAGAZINES\n",
       "1       MEDIA_AND_VIDEO\n",
       "Name: Category, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_cat = appdf.Category\n",
    "col_cat.head(2)d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'Category',\n",
       " 'Score',\n",
       " 'Description',\n",
       " 'Price',\n",
       " 'PublicationDate',\n",
       " 'AppSize',\n",
       " 'Name',\n",
       " 'ContentRating',\n",
       " 'LastUpdateDate',\n",
       " 'Instalations',\n",
       " 'IsTopDeveloper',\n",
       " 'HaveInAppPurchases',\n",
       " 'IsFree',\n",
       " 'Developer']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "appdf.columns.values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['NEWS_AND_MAGAZINES', 'MEDIA_AND_VIDEO', 'ENTERTAINMENT', 'FINANCE',\n",
       "       'MUSIC_AND_AUDIO', 'TRAVEL_AND_LOCAL', 'EDUCATION', 'BUSINESS',\n",
       "       'PERSONALIZATION', 'TRANSPORTATION', 'SPORTS', 'SOCIAL',\n",
       "       'COMMUNICATION', 'PHOTOGRAPHY', 'LIFESTYLE', 'HEALTH_AND_FITNESS',\n",
       "       'TOOLS', 'PRODUCTIVITY', 'WEATHER', 'BOOKS_AND_REFERENCE',\n",
       "       'GAME_TRIVIA', 'MEDICAL', 'GAME_PUZZLE', 'GAME_CASUAL', 'SHOPPING',\n",
       "       'GAME_MUSIC', 'GAME_ACTION', 'GAME_ARCADE', 'GAME_SIMULATION',\n",
       "       'GAME_CARD', 'GAME_CASINO', 'LIBRARIES_AND_DEMO',\n",
       "       'GAME_EDUCATIONAL', 'GAME_SPORTS', 'GAME_WORD', 'GAME_RACING',\n",
       "       'GAME_ROLE_PLAYING', 'GAME_BOARD', 'COMICS', 'GAME_STRATEGY',\n",
       "       'GAME_ADVENTURE'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(col_cat.unique())\n",
    "col_cat.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''Use StemmedCountVectorizer to do:\n",
    "1. lower casing the raw post in the preprossing step done in parent calss.\n",
    "2. Extracting all individual words in the tokenization step in parent class.\n",
    "3. Converting each word into its stemmed version.'''\n",
    "import nltk.stem\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "        analyzer = super(StemmedCountVectorizer,self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Returns a vectorized ND dataframe, vectorized ndarray, and an instance of the vectorizer Class used to transform.'''\n",
    "def vectorize_column(dataframe,column_name,vectorizer=None):\n",
    "    if vectorizer is None:\n",
    "        print(\"No Vectorizer is explicitly specified. Using CountVectorizer as default one. \")\n",
    "        column_vectorizer = CountVectorizer(min_df=1)\n",
    "    else:\n",
    "        column_vectorizer = vectorizer\n",
    "    \n",
    "    if column_name in dataframe.columns.values.tolist():\n",
    "        column_df = dataframe[column_name] #select all the samples from the column passed as param.\n",
    "        fmatrix = column_vectorizer.fit_transform(column_df) #convert text features to numerical vectors\n",
    "        #print column_vectorizer.get_feature_names() #print the features names formed after fitting a categorical data\n",
    "        #print(\"vectorized into matrix of shape\"), fmatrix.toarray().shape\n",
    "        dataframe_f = pd.DataFrame(fmatrix.toarray(), columns=column_vectorizer.get_feature_names())\n",
    "        print(\"formed dataframe of size:(\"),dataframe_f.index.max()+1,\",\", dataframe_f.head(1).shape[1],\")\"\n",
    "        \n",
    "        return dataframe_f, fmatrix, column_vectorizer\n",
    "    else:\n",
    "        print(\"No column found\")\n",
    "        \n",
    "'''Returns a vectorized ND dataframe, vectorized ndarray, and an instance of the vectorizer Class used to transform.'''\n",
    "def vectorize_column2(dataframe,column_name,vectorizer=None, n_samples=None):\n",
    "    if vectorizer is None:\n",
    "        print(\"No Vectorizer is explicitly specified. Using CountVectorizer as default one. \")\n",
    "        column_vectorizer = CountVectorizer(min_df=1)\n",
    "    else:\n",
    "        column_vectorizer = vectorizer\n",
    "    \n",
    "    if column_name in dataframe.columns.values.tolist():\n",
    "        \n",
    "        if n_samples is None:\n",
    "            column_df = dataframe[column_name] #select all the samples from the column passed as param. \n",
    "            print len(column_df)\n",
    "        else:\n",
    "            column_df = dataframe[column_name].iloc[0:n_samples] #select all the samples from the column passed as param.\n",
    "            print len(column_df)\n",
    "\n",
    "        fmatrix = column_vectorizer.fit_transform(column_df) #convert text features to numerical vectors\n",
    "        #print column_vectorizer.get_feature_names() #print the features names formed after fitting a categorical data\n",
    "        #print(\"vectorized into matrix of shape\"), fmatrix.toarray().shape\n",
    "        dataframe_f = pd.DataFrame(fmatrix.toarray(), columns=column_vectorizer.get_feature_names())\n",
    "        print(\"formed dataframe of size:(\"),dataframe_f.index.max()+1,\",\", dataframe_f.head(1).shape[1],\")\"\n",
    "        \n",
    "        return dataframe_f, fmatrix, column_vectorizer\n",
    "    else:\n",
    "        print(\"No column found\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "formed dataframe of size:( 100000 , 41 )\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>books_and_refer</th>\n",
       "      <th>busi</th>\n",
       "      <th>comic</th>\n",
       "      <th>communic</th>\n",
       "      <th>educ</th>\n",
       "      <th>entertain</th>\n",
       "      <th>financ</th>\n",
       "      <th>game_act</th>\n",
       "      <th>game_adventur</th>\n",
       "      <th>game_arcad</th>\n",
       "      <th>...</th>\n",
       "      <th>person</th>\n",
       "      <th>photographi</th>\n",
       "      <th>product</th>\n",
       "      <th>shop</th>\n",
       "      <th>social</th>\n",
       "      <th>sport</th>\n",
       "      <th>tool</th>\n",
       "      <th>transport</th>\n",
       "      <th>travel_and_loc</th>\n",
       "      <th>weather</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   books_and_refer  busi  comic  communic  educ  entertain  financ  game_act  \\\n",
       "0                0     0      0         0     0          0       0         0   \n",
       "1                0     0      0         0     0          0       0         0   \n",
       "2                0     0      0         0     0          1       0         0   \n",
       "3                0     0      0         0     0          0       1         0   \n",
       "4                0     0      0         0     0          0       0         0   \n",
       "\n",
       "   game_adventur  game_arcad   ...     person  photographi  product  shop  \\\n",
       "0              0           0   ...          0            0        0     0   \n",
       "1              0           0   ...          0            0        0     0   \n",
       "2              0           0   ...          0            0        0     0   \n",
       "3              0           0   ...          0            0        0     0   \n",
       "4              0           0   ...          0            0        0     0   \n",
       "\n",
       "   social  sport  tool  transport  travel_and_loc  weather  \n",
       "0       0      0     0          0               0        0  \n",
       "1       0      0     0          0               0        0  \n",
       "2       0      0     0          0               0        0  \n",
       "3       0      0     0          0               0        0  \n",
       "4       0      0     0          0               0        0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Possible type of count vectorizer that could be used.\n",
    "    Examples: \n",
    "    column_vectorizer = CountVectorizer(min_df=1)\n",
    "    column_vectorizer = CountVectorizer(min_df =1, stop_words='english') \n",
    "    print column_vectorizer.get_feature_names()\n",
    "'''\n",
    "\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "newfeature, fmatrix, column_vectorizer = vectorize_column(appdf, 'Category', stem_vectorizer)\n",
    "newfeature.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''Computes eculidean distance between two normalized vectors v1 and v2'''\n",
    "def dist_norm(v1,v2):\n",
    "    v1_normalized = v1/sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2/sp.linalg.norm(v2.toarray())\n",
    "    delta= v1_normalized - v2_normalized\n",
    "    return sp.linalg.norm(delta.toarray()) #norm() calculates the Eculidean norm i.e. shortest distance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### '''Analysis of 'Category' data-columns\n",
    "#Each of the application has only one 'category' so the each of the category is equi-distance from all other.\n",
    "Though similarity of each of the values of category is same, the category-name itself might not effect the rating equally.\n",
    "That is why they are inluded as training features.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "best_doc = None\n",
    "best_i = None\n",
    "\n",
    "def best_match(column_vectorizer, fmatrix,text_to_compare):\n",
    "    n_samples = 100 # fmatrix.shape[0]\n",
    "    best_dist = sys.maxint\n",
    "    vect_to_compare = column_vectorizer.transform(text_to_compare)\n",
    "    for i in range(0, n_samples):\n",
    "        text_in_column = col_cat[i]\n",
    "        if text_in_column == text_to_compare[0]:\n",
    "            continue\n",
    "        vector_for_column_text = fmatrix.getrow(i)\n",
    "        #d = dist_raw(post_vec, new_post_vec)\n",
    "        d = dist_norm(vector_for_column_text, vect_to_compare)\n",
    "        print \"===Category of app- %i with dist = %.2f: %s\"%(i,d,text_in_column)\n",
    "        if d < best_dist:\n",
    "            best_dist = d\n",
    "            best_i = i\n",
    "    print \"Best text in category is %i with dist = %.4f\"%(best_i,best_dist)\n",
    "print type([col_cat[4]])\n",
    "#best_match(column_vectorizer,fmatrix, [col_cat[4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### #Analysis of Description Field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stat_vectorized_matrix(vectorized_array,vectorizer_type=None):\n",
    "   \n",
    "    #count the number of features generated,\n",
    "    print type(vectorized_array)\n",
    "    m, n = vectorized_array.shape\n",
    "    count_non_zero_cells = np.count_nonzero(vectorized_array)\n",
    "    print(\"vectorizer_type:\"),type(column_vectorizer)\n",
    "    print(\"number of features generated: \"), n\n",
    "    print(\"n_samples:\"), m\n",
    "    #count the number of non-zero entries,\n",
    "    print(\"% of non-zero values:\"), count_non_zero_cells/float(m*n) * 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           Description\n",
      "280  臺灣北中南東怎麼玩才最深入？旅遊行程該怎麼安排才最盡興？您不可不知的巷內美食、私房景點以及特...\n",
      "281  Taiwan GuideWithMe is an offline travel guide,...\n",
      "282  「旅行台灣」是交通部觀光局提供免費在地導覽程式。導覽內容包含美食、住宿、景點、商家等資訊。並...\n",
      "283                            Bus tracker for Taiwan.\n",
      "284  Now you do not need a physical radio through t...\n",
      "5\n",
      "formed dataframe of size:( 5 , 286 )\n",
      "feature_names [u'02', u'04', u'160', u'2014', u'50\\u5f35\\u5ba2\\u88fd\\u5730\\u5716\\u5c0e\\u89bd\\u53ca\\u885b\\u661f\\u5b9a\\u4f4d\\u670d\\u52d9', u'90', u'accord', u'action', u'ad', u'addit', u'advanc', u'adventur', u'advic', u'amend', u'android', u'app', u'apphistori', u'articl', u'aspir', u'asus', u'augment', u'automat', u'autoplay', u'avail', u'base', u'batteri', u'beaten', u'broadcast', u'browsersg', u'bus', u'cancel', u'care', u'cellular', u'channel', u'channelsi', u'channelsmost', u'check', u'chosen', u'citi', u'clean', u'click', u'cn\\u5feb\\u4f86\\u4e0b\\u8f09', u'com', u'come', u'complet', u'constant', u'consumptionb', u'contain', u'continu', u'cost', u'countri', u'cover', u'current', u'data', u'descend', u'destin', u'detail', u'develop', u'differ', u'discov', u'doe', u'dropdown', u'easi', u'end', u'enjoy', u'essenti', u'expect', u'facebook', u'favorit', u'featur', u'fee', u'feedback', u'fi', u'fix', u'follow', u'franc', u'friend', u'function', u'fun\\u4ea4\\u901a', u'fun\\u597d\\u79ae', u'fun\\u71b1\\u9ede', u'fun\\u80cc\\u5305', u'fun\\u81fa\\u7063', u'fun\\u884c\\u7a0b', u'general', u'googl', u'gps', u'guid', u'guidewithm', u'hang', u'hawaii', u'headset', u'healthi', u'help', u'hongkong', u'hotel', u'http', u'https', u'id', u'illustr', u'imag', u'improv', u'incom', u'increas', u'info', u'instal', u'interfac', u'itali', u'item', u'japan', u'lazio', u'leav', u'list', u'listen', u'listen2014', u'local', u'locat', u'map', u'market', u'master', u'menu', u'model', u'money', u'multipl', u'nation', u'need', u'network', u'notif', u'offc', u'offer', u'offlin', u'onlinenot', u'open', u'operatef', u'option', u'order', u'outgo', u'part', u'paus', u'phone', u'phrase', u'physic', u'pictur', u'place', u'play', u'playback', u'playd', u'plus', u'pocket', u'poi', u'power', u'program', u'project', u'propos', u'provid', u'radio', u'rate', u'realiti', u'recommend', u'reduc', u'refer', u'region', u'relat', u'reliabl', u'restaur', u'return', u'review', u'rich', u'road', u'roam', u'rule', u'russia', u'safe', u'save', u'saver', u'search', u'section', u'send', u'set', u'share', u'similar', u'simpl', u'sleep', u'slight', u'sourc', u'spain', u'stabil', u'standbi', u'state', u'station', u'status', u'stay', u'stop', u'stop1', u'store', u'suggest', u'suit', u'sundanc', u'switch', u'switzerland', u'tablet', u'taichung', u'tainan', u'taipei', u'taiwan', u'taiwan_go', u'taoyuan', u'tast', u'tax', u'thank', u'time', u'timer', u'topic', u'track', u'tracker', u'travel', u'turkey', u'tv', u'tw', u'twitter', u'unplug', u'unus', u'updat', u'usag', u'use', u'varieti', u'view', u'vivid', u'wait', u'warn', u'welcom', u'white', u'wi', u'wifi', u'wikivoyag', u'world', u'writer', u'www', u'you2', u'zenfon', u'\\u4e00\\u8d77taiwan', u'\\u4e26\\u7d50\\u5408\\u64f4\\u589e\\u5be6\\u5883', u'\\u4e26\\u975e\\u4f9d\\u9053\\u8def\\u898f\\u5283\\u8ddd\\u96e2', u'\\u4e5f\\u80fd\\u900f\\u904e\\u885b\\u661f\\u5b9a\\u4f4d', u'\\u4ea4\\u901a\\u6307\\u5357\\u5354\\u52a9\\u60a8\\u65c5\\u904a\\u4e0d\\u8ff7\\u8def5', u'\\u4eab\\u53d7\\u865b\\u64ec\\u5c0e\\u904a\\u670d\\u52d9', u'\\u4f4f\\u5bbf', u'\\u5132\\u5b58\\u6700\\u611b\\u666f\\u9ede\\u5e97\\u5bb6\\u53ca\\u641c\\u96c6\\u500b\\u4eba\\u8b77\\u7167\\u5fbd\\u7ae0', u'\\u5275\\u9020\\u500b\\u4eba\\u5c08\\u5c6cpassport', u'\\u5373\\u53ef\\u7372\\u5f97', u'\\u53ef\\u4ee5\\u6253\\u5361\\u641c\\u96c6\\u96b1\\u85cf\\u7248\\u5fbd\\u7ae0', u'\\u5546\\u5bb6\\u7b49\\u8cc7\\u8a0a', u'\\u56b4\\u9078\\u5728\\u5730\\u5403\\u4f4f\\u770b\\u73a9\\u8cb7\\u71b1\\u9ede6', u'\\u56b4\\u9078\\u81fa\\u7063\\u5317\\u4e2d\\u5357\\u677150\\u5927\\u666f\\u9ede', u'\\u5c0e\\u89bd\\u5167\\u5bb9\\u5305\\u542b\\u7f8e\\u98df', u'\\u60a8\\u4e0d\\u53ef\\u4e0d\\u77e5\\u7684\\u5df7\\u5167\\u7f8e\\u98df', u'\\u6191\\u8457', u'\\u6211\\u7684\\u8b77\\u7167', u'\\u63a8\\u85a6\\u5404\\u666f\\u9ede\\u6700\\u4f73\\u53c3\\u89c0\\u6642\\u9593', u'\\u63a8\\u85a6\\u6771\\u90e8\\u53ca\\u897f\\u90e8\\u7279\\u8272\\u8def\\u7dda3', u'\\u63d0\\u4f9b\\u65b9\\u5411\\u5c0e\\u5f15\\u548c\\u8def\\u5f91\\u898f\\u5283', u'\\u65c5\\u884c\\u53f0\\u7063', u'\\u65c5\\u904a\\u884c\\u7a0b\\u8a72\\u600e\\u9ebc\\u5b89\\u6392\\u624d\\u6700\\u76e1\\u8208', u'\\u65c5\\u904a\\u9054\\u4eba\\u5e6b\\u60a8\\u53bb\\u856a\\u5b58\\u83c1', u'\\u662f\\u4ea4\\u901a\\u90e8\\u89c0\\u5149\\u5c40\\u63d0\\u4f9b\\u514d\\u8cbb\\u5728\\u5730\\u5c0e\\u89bd\\u7a0b\\u5f0f', u'\\u666f\\u9ede', u'\\u666f\\u9ede\\u8ddd\\u96e2', u'\\u672c\\u7a0b\\u5f0f\\u4e2d', u'\\u70ba\\u65c5\\u7a0b\\u5beb\\u4e0b\\u5b8c\\u7f8e\\u8a18\\u9304', u'\\u70ba\\u9ede\\u8207\\u9ede\\u7684\\u76f4\\u7dda\\u8ddd\\u96e2', u'\\u751f\\u52d5\\u5716\\u7247\\u63d0\\u4f9b\\u60a8\\u6700\\u9802\\u5c16\\u7684\\u65c5\\u904a\\u4ecb\\u7d39\\u5537', u'\\u7684\\u8173\\u6b65', u'\\u79c1\\u623f\\u666f\\u9ede\\u4ee5\\u53ca\\u7279\\u8272\\u4f34\\u624b', u'\\u7c21\\u9ad4\\u7248\\u73fe\\u6b63\\u767c\\u884c', u'\\u7cbe\\u5fc3\\u7e6a\\u88fd\\u5177\\u6709\\u81fa\\u7063\\u7279\\u8272\\u7684\\u5ba2\\u88fd\\u5730\\u5716', u'\\u7cbe\\u9078\\u5728\\u5730\\u512a\\u8cea\\u4f34\\u624b\\u597d\\u79ae\\u63a8\\u85a64', u'\\u7db2\\u7f85\\u6700\\u5177\\u81fa\\u7063\\u5473\\u7684\\u5e97\\u5bb6\\u53ca\\u7d55\\u5c0d\\u4e0d\\u80fd\\u932f\\u904e\\u7684\\u62db\\u724c\\u5546\\u54c1', u'\\u81fa\\u7063\\u500b\\u4eba\\u904a', u'\\u81fa\\u7063\\u5317\\u4e2d\\u5357\\u6771\\u600e\\u9ebc\\u73a9\\u624d\\u6700\\u6df1\\u5165', u'\\u8acb\\u4f9d\\u5be6\\u969b\\u9053\\u8def\\u72c0\\u6cc1\\u884c\\u8d70', u'\\u8b93\\u611b\\u81ea\\u52a9\\u65c5\\u884c\\u7684\\u60a8\\u5373\\u4fbf\\u624b\\u6a5f\\u6c92\\u6709\\u884c\\u52d5\\u4e0a\\u7db2', u'\\u900f\\u904e\\u6d3b\\u6f51\\u7b46\\u89f8', u'\\u9019\\u9ebc\\u591a\\u7cbe\\u5f69\\u7684\\u65c5\\u904a\\u8cc7\\u8a0a', u'\\u9084\\u4e0d\\u5feb\\u9ede\\u8ddf\\u8457', u'\\u9084\\u6709', u'\\u9678\\u5ba2\\u4f86\\u53f0\\u81ea\\u7531\\u884c\\u851a\\u70ba\\u98a8\\u6f6e']\n",
      "<type 'numpy.ndarray'>\n",
      "vectorizer_type: <class '__main__.StemmedCountVectorizer'>\n",
      "number of features generated:  286\n",
      "n_samples: 5\n",
      "% of non-zero values: 21.3286713287\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>02</th>\n",
       "      <th>04</th>\n",
       "      <th>160</th>\n",
       "      <th>2014</th>\n",
       "      <th>50張客製地圖導覽及衛星定位服務</th>\n",
       "      <th>90</th>\n",
       "      <th>accord</th>\n",
       "      <th>action</th>\n",
       "      <th>ad</th>\n",
       "      <th>addit</th>\n",
       "      <th>...</th>\n",
       "      <th>網羅最具臺灣味的店家及絕對不能錯過的招牌商品</th>\n",
       "      <th>臺灣個人遊</th>\n",
       "      <th>臺灣北中南東怎麼玩才最深入</th>\n",
       "      <th>請依實際道路狀況行走</th>\n",
       "      <th>讓愛自助旅行的您即便手機沒有行動上網</th>\n",
       "      <th>透過活潑筆觸</th>\n",
       "      <th>這麼多精彩的旅遊資訊</th>\n",
       "      <th>還不快點跟著</th>\n",
       "      <th>還有</th>\n",
       "      <th>陸客來台自由行蔚為風潮</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 286 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   02  04  160  2014  50張客製地圖導覽及衛星定位服務  90  accord  action  ad  addit  \\\n",
       "0   0   0    0     0                 1   0       0       0   0      0   \n",
       "1   0   0    1     0                 0   1       0       0   0      0   \n",
       "2   0   0    0     0                 0   0       0       0   0      0   \n",
       "3   0   0    0     0                 0   0       0       0   0      0   \n",
       "4   1   3    0     1                 0   0       1       1   2      1   \n",
       "\n",
       "      ...       網羅最具臺灣味的店家及絕對不能錯過的招牌商品  臺灣個人遊  臺灣北中南東怎麼玩才最深入  請依實際道路狀況行走  \\\n",
       "0     ...                            1      3              1           0   \n",
       "1     ...                            0      0              0           0   \n",
       "2     ...                            0      0              0           1   \n",
       "3     ...                            0      0              0           0   \n",
       "4     ...                            0      0              0           0   \n",
       "\n",
       "   讓愛自助旅行的您即便手機沒有行動上網  透過活潑筆觸  這麼多精彩的旅遊資訊  還不快點跟著  還有  陸客來台自由行蔚為風潮  \n",
       "0                   1       1           1       1   2            1  \n",
       "1                   0       0           0       0   0            0  \n",
       "2                   0       0           0       0   0            0  \n",
       "3                   0       0           0       0   0            0  \n",
       "4                   0       0           0       0   0            0  \n",
       "\n",
       "[5 rows x 286 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_desc = appdf.Description\n",
    "\n",
    "df_desc = pd.DataFrame(col_desc, columns=['Description']).iloc[280:285]\n",
    "print df_desc.to_string()\n",
    "df_desc.to_csv(\"test_file.csv\",sep=\",\")\n",
    "\n",
    "stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "newfeature, fmatrix, column_vectorizer = vectorize_column2(df_desc, 'Description', stem_vectorizer, n_samples=100)\n",
    "print(\"feature_names\"), column_vectorizer.get_feature_names()\n",
    "stat_vectorized_matrix(fmatrix.toarray(), column_vectorizer)\n",
    "newfeature.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In an average, one cell of a description column generates 57 features. In this way, there are 57*n_samples features geneated after vectorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of Name Field\n",
    "    suggest some of the price for higer number of sale\n",
    "    w1: parameterized loudness of words in context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000     5000+ Cute Love SMS Collection\n",
      "1003      30000+ Funny Jokes Collection\n",
      "1007     90000+ SMS Messages Collection\n",
      "1008     75000+ SMS Messages Collection\n",
      "1578             DuPont™ Tychem® 4000 S\n",
      "2591        Learn Ukrainian 6,000 Words\n",
      "3945                      Sudoku 10'000\n",
      "3948                 Sudoku 10'000 Free\n",
      "3953                 Sudoku 10'000 Plus\n",
      "4350                     1000 Aventuras\n",
      "6369     フルル大辞典 ～即引き！略語・用語・薬辞典 10,000語～\n",
      "6401                 MedCalc 3000 中文精华版\n",
      "7228        プリンス育成☆マジLOVE3000％ for うたプリ\n",
      "7322                20000 Leagues Slots\n",
      "9173             Indian Recipes 10.000+\n",
      "9320      2000 Piadas Engraçadas Brasil\n",
      "10048    50000 Status Quotes Collection\n",
      "10251                      鬼監督の1000本ノック\n",
      "11380             1000 Recipes in Hindi\n",
      "12334           Desert Race Toyota 1000\n",
      "13523                     Taxxi 4209000\n",
      "13630                مسجات +15000 رسالة\n",
      "13702                    Solitaire 1000\n",
      "14624         Learn Chinese 6,000 Words\n",
      "14627      Learn Chinese 10000 Mandarin\n",
      "15473     Ackmi 2000s Music Trivia Quiz\n",
      "16001        1000+ Mignon SMS Française\n",
      "16037         BigOven: 350,000+ Recipes\n",
      "16062      3000 Adbhut Rahasya in HIndi\n",
      "16478           2000+ Tamil Kavithaigal\n",
      "                      ...              \n",
      "78143     13000 videos english learning\n",
      "78338              1000000+ FREE Ebooks\n",
      "78736      Chinese Library - 2000 Books\n",
      "78739    Hemmabiblioteket - 1000 Böcker\n",
      "78984      1000+ Business Idea and Fund\n",
      "80514          Learn German 6,000 Words\n",
      "81018       Best WhatsApp Status 10000+\n",
      "81252          House of 1000 Doors Full\n",
      "82519                 English 6000 Free\n",
      "82521                 ESPANOL 6000 Free\n",
      "85350         꽁돈 - 3분3000원,문상,틴캐시,돈버는어플\n",
      "85526          かわいい！顔文字9000+（無料かおもじアプリ）\n",
      "85879     Бир кеча кундузда 1000 суннат\n",
      "85976    100000 SMS Collection & Status\n",
      "85979        71000+ Hindi Shayari Dukan\n",
      "85994       50000+ Days SMS Collections\n",
      "85997    25000+ SMS Messages Collection\n",
      "87011              100000+ SMS Messages\n",
      "87779            +1000 Hindi SMS Shayri\n",
      "87970    Zinio: 5000+ Digital Magazines\n",
      "90799            Learn Thai 6,000 Words\n",
      "90871           Learn Greek 6,000 Words\n",
      "91182     真人发音 - 英文5分钟(Eng5)-收纳超过5000例句\n",
      "92206        FooBar 2000 Remote Control\n",
      "92742                       iTYDOM 1000\n",
      "93848                 Teleprompter 2000\n",
      "95193    NonogramZ 1000+ online puzzles\n",
      "95900               1000 Cookie Recipes\n",
      "96105        House of 1000 Doors 2 Free\n",
      "98087                   1000 Adventures\n",
      "Name: Name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "col_name = appdf.Name\n",
    "print col_name[col_name.str.contains('000')]\n",
    "\n",
    "\n",
    "#stem_vectorizer = StemmedCountVectorizer(min_df =1, stop_words='english')\n",
    "#newfeature, fmatrix, column_vectorizer = vectorize_column(appdf, 'Name', stem_vectorizer)\n",
    "#print column_vectorizer.get_feature_names()\n",
    "#newfeature.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analysis of 'Instalations'\n",
    "    It is range values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        150\n",
      "1      15000\n",
      "2       6000\n",
      "3    1500000\n",
      "4       6000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "col_name = appdf.Instalations\n",
    "def separate_instalation_column(dataframe, column_name,return_data_type_as=None):\n",
    "    \n",
    "    col_name = appdf[column_name]\n",
    "    ls = col_name.str.split('-').str.get(0).str.strip(' ').str.replace(',','') #series object\n",
    "    hs = col_name.str.split('-').str.get(1).str.strip(' ').str.replace(',','') #series object\n",
    "    \n",
    "    if return_data_type_as is float64:\n",
    "        ls = ls.astype(float).fillna(0.0)\n",
    "        hs = hs.astype(float).fillna(0.0)\n",
    "        return ls, hs\n",
    "    else:\n",
    "        return ls, hs\n",
    "    \n",
    "ls, hs = separate_instalation_column(appdf,'Instalations', float64)\n",
    "appdf.installs_ls = ls\n",
    "appdf.installs_hs = hs\n",
    "print appdf.installs_ls.head(5) + appdf.installs_hs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Issues:\n",
    "            Tokenization problem\n",
    "            Vectorization problem memory error\n",
    "            Plot the frequency distribution of price \n",
    "            Do linear regression on price and plot Predicted_price-Desired_price Vs Predicted_price"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
